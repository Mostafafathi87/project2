{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc39377-12ae-490b-bae4-c755b3ac18b4",
   "metadata": {},
   "source": [
    "# Using The Super Resolution Convolutional Neural Network for Image Restoration\r\n",
    "Welcome to this tutorial on single-image super-resolution. The goal of super-resolution (SR) is to recover a high resolution image from a low resolution input, or as they might say on any modern crime show, enhance!\r\n",
    "\r\n",
    "To accomplish this goal, we will be deploying the super-resolution convolution neural network (SRCNN) using Keras. This network was published in the paper, \"Image Super-Resolution Using Deep Convolutional Networks\" by Chao Dong, et al. in 2014. You can read the full paper at https://arxiv.org/abs/1501.00092.\r\n",
    "\r\n",
    "As the title suggests, the SRCNN is a deep convolutional neural network that learns end-to-end mapping of low resolution to high resolution images. As a result, we can use it to improve the image quality of low resolution images. To evaluate the performance of this network, we will be using three image quality metrics: peak signal to noise ratio (PSNR), mean squared error (MSE), and the structural similarity (SSIM) index.\r\n",
    "\r\n",
    "Furthermore, we will be using OpenCV, the Open Source Computer Vision Library. OpenCV was originally developed by Intel and is used for many real-time computer vision applications. In this particular project, we will be using it to pre and post process our images. As you will see later, we will frequently be converting our images back and forth between the RGB, BGR, and YCrCb color spaces. This is necessary because the SRCNN network was trained on the luminance (Y) channel in the YCrCb color space.\r\n",
    "\r\n",
    "During this project, you will learn how to:\r\n",
    "\r\n",
    "use the PSNR, MSE, and SSIM image quality metrics,\r\n",
    "process images using OpenCV,\r\n",
    "convert between the RGB, BGR, and YCrCb color spaces,\r\n",
    "build deep neural networks in Keras,\r\n",
    "deploy and evaluate the SRCNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfa99b64-eaf5-45d0-b8a2-645e2d425f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\n",
      "Keras: 3.6.0\n",
      "OpenCV: 4.10.0\n",
      "NumPy: 1.26.4\n",
      "Matplotlib: 3.8.0\n",
      "Scikit-Image: 0.22.0\n"
     ]
    }
   ],
   "source": [
    "# check package versions\n",
    "import sys\n",
    "import keras\n",
    "import cv2\n",
    "import numpy\n",
    "import matplotlib\n",
    "import skimage\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('Keras: {}'.format(keras.__version__))\n",
    "print('OpenCV: {}'.format(cv2.__version__))\n",
    "print('NumPy: {}'.format(numpy.__version__))\n",
    "print('Matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('Scikit-Image: {}'.format(skimage.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c92c8a2-17ed-44b0-baba-8ec31c110883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "# python magic function, displays pyplot figures in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c75ce4c-2efc-4361-940c-ee5eaf0b47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for peak signal-to-noise ratio (PSNR)\n",
    "def psnr(target, ref):\n",
    "         \n",
    "    # assume RGB/BGR image\n",
    "    target_data = target.astype(float)\n",
    "    ref_data = ref.astype(float)\n",
    "\n",
    "    diff = ref_data - target_data\n",
    "    diff = diff.flatten('C')\n",
    "\n",
    "    rmse = np.sqrt(np.mean(diff ** 2.))\n",
    "\n",
    "    return 20 * math.log10(255. / rmse)\n",
    "\n",
    "# define function for mean squared error (MSE)\n",
    "def mse(target, ref):\n",
    "    # the MSE between the two images is the sum of the squared difference between the two images\n",
    "    err = np.sum((target.astype('float') - ref.astype('float')) ** 2)\n",
    "    err /= float(target.shape[0] * target.shape[1])\n",
    "    \n",
    "    return err\n",
    "\n",
    "# define function that combines all three image quality metrics\n",
    "def compare_images(target, ref, win_size=5):\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse_value = mse(target, ref)\n",
    "    \n",
    "    # Peak Signal-to-Noise Ratio (PSNR)\n",
    "    if mse_value == 0:  # identical images\n",
    "        psnr_value = 100\n",
    "    else:\n",
    "        max_pixel = 255.0\n",
    "        psnr_value = 20 * np.log10(max_pixel / np.sqrt(mse_value))\n",
    "    \n",
    "    # Structural Similarity Index (SSIM)\n",
    "    ssim_value, _ = ssim(target, ref, full=True, multichannel=True, win_size=win_size, channel_axis=2)\n",
    "    \n",
    "    return psnr_value, mse_value, ssim_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dde1ae-e499-4134-9006-2a5a7fda24d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c7027a2-9d80-455c-b7fd-8748ef59562a",
   "metadata": {},
   "source": [
    " http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73a799bd-20e6-40bf-9d10-bda06ad63237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare degraded images by introducing quality distortions via resizing\n",
    "\n",
    "def prepare_images(path, factor):\n",
    "    \n",
    "    # loop through the files in the directory\n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        # open the file\n",
    "        img = cv2.imread(path + '/' + file)\n",
    "        \n",
    "        # find old and new image dimensions\n",
    "        h, w, c = img.shape\n",
    "        new_height = int(h / factor)  # Convert to integer\n",
    "        new_width = int(w / factor)   # Convert to integer\n",
    "        \n",
    "        # resize the image - down\n",
    "        img = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_LINEAR)\n",
    "        \n",
    "        # resize the image - up\n",
    "        img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n",
    "        \n",
    "        # save the image\n",
    "        print('Saving {}'.format(file))\n",
    "        cv2.imwrite('images/{}'.format(file), img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f053850c-2022-4fb4-affe-6b946eba57a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving baboon.bmp\n",
      "Saving baby_GT.bmp\n",
      "Saving barbara.bmp\n",
      "Saving bird_GT.bmp\n",
      "Saving butterfly_GT.bmp\n",
      "Saving coastguard.bmp\n",
      "Saving comic.bmp\n",
      "Saving face.bmp\n",
      "Saving flowers.bmp\n",
      "Saving foreman.bmp\n",
      "Saving head_GT.bmp\n",
      "Saving lenna.bmp\n",
      "Saving monarch.bmp\n",
      "Saving pepper.bmp\n",
      "Saving ppt3.bmp\n",
      "Saving woman_GT.bmp\n",
      "Saving zebra.bmp\n"
     ]
    }
   ],
   "source": [
    "prepare_images('source1/', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27180564-162c-48d3-b8d0-7adbaaf2d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the SRCNN model\n",
    "def model():\n",
    "    \n",
    "    # define model type\n",
    "    SRCNN = Sequential()\n",
    "    \n",
    "    # add model layers\n",
    "    SRCNN.add(Conv2D(filters=128, kernel_size = (9, 9), kernel_initializer='glorot_uniform',\n",
    "                     activation='relu', padding='valid', use_bias=True, input_shape=(None, None, 1)))\n",
    "    SRCNN.add(Conv2D(filters=64, kernel_size = (3, 3), kernel_initializer='glorot_uniform',\n",
    "                     activation='relu', padding='same', use_bias=True))\n",
    "    SRCNN.add(Conv2D(filters=1, kernel_size = (5, 5), kernel_initializer='glorot_uniform',\n",
    "                     activation='linear', padding='valid', use_bias=True))\n",
    "    \n",
    "    # define optimizer\n",
    "    adam = Adam(learning_rate=0.0003)\n",
    "    \n",
    "    # compile model\n",
    "    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return SRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138bd0b-a52d-4712-9a8b-5516253d0b04",
   "metadata": {},
   "source": [
    "https://github.com/MarkPrecursor/SRCNN-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43d0cfcf-b26d-474a-8124-01151bfd8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define necessary image processing functions\n",
    "\n",
    "def modcrop(img, scale):\n",
    "    tmpsz = img.shape\n",
    "    sz = tmpsz[0:2]\n",
    "    sz = sz - np.mod(sz, scale)\n",
    "    img = img[0:sz[0], 1:sz[1]]\n",
    "    return img\n",
    "\n",
    "\n",
    "def shave(image, border):\n",
    "    img = image[border: -border, border: -border]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a1e81df-5d5c-4f1c-afb8-dee665ecb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main prediction function\n",
    "def predict(image_path):\n",
    "    try:\n",
    "        # Load the srcnn model with weights\n",
    "        srcnn = model()\n",
    "        srcnn.load_weights('3051crop_weight_200.h5')\n",
    "\n",
    "        # Load the degraded and reference images\n",
    "        path, file = os.path.split(image_path)\n",
    "        degraded = cv2.imread(image_path)\n",
    "        ref = cv2.imread('source1/{}'.format(file))\n",
    "\n",
    "        # Ensure images are loaded correctly\n",
    "        if degraded is None or ref is None:\n",
    "            print(f\"Error: Image(s) not found for {file}\")\n",
    "            return None\n",
    "\n",
    "        # Preprocess the image with modcrop\n",
    "        ref = modcrop(ref, 3)\n",
    "        degraded = modcrop(degraded, 3)\n",
    "\n",
    "        # Convert the image to YCrCb (SRCNN is trained on Y channel)\n",
    "        temp = cv2.cvtColor(degraded, cv2.COLOR_BGR2YCrCb)\n",
    "        \n",
    "        # Normalize and prepare input\n",
    "        Y = np.zeros((1, temp.shape[0], temp.shape[1], 1), dtype=float)\n",
    "        Y[0, :, :, 0] = temp[:, :, 0].astype(float) / 255\n",
    "\n",
    "        # Perform super-resolution with srcnn\n",
    "        pre = srcnn.predict(Y, batch_size=1)\n",
    "\n",
    "        # Post-process output\n",
    "        pre *= 255\n",
    "        pre[pre[:] > 255] = 255\n",
    "        pre[pre[:] < 0] = 0\n",
    "        pre = pre.astype(np.uint8)\n",
    "\n",
    "        # Copy Y channel back to image and convert to BGR\n",
    "        temp = shave(temp, 6)\n",
    "        temp[:, :, 0] = pre[0, :, :, 0]\n",
    "        output = cv2.cvtColor(temp, cv2.COLOR_YCrCb2BGR)\n",
    "        \n",
    "        # Remove border from reference and degraded image\n",
    "        ref = shave(ref.astype(np.uint8), 6)\n",
    "        degraded = shave(degraded.astype(np.uint8), 6)\n",
    "\n",
    "        # Image quality calculations\n",
    "        scores = []\n",
    "        scores.append(compare_images(degraded, ref))\n",
    "        scores.append(compare_images(output, ref))\n",
    "\n",
    "        # Return images and scores\n",
    "        return ref, degraded, output, scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in predict function: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64e8e327-4ea1-431b-aa9c-06d07e08df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in predict function: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n",
      "> Overload resolution failed:\n",
      ">  - src is not a numpy array, neither a scalar\n",
      ">  - Expected Ptr<cv::UMat> for argument 'src'\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ref, degraded, output, scores \u001b[38;5;241m=\u001b[39m predict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/flowers.bmp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print all scores for all images\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDegraded Image: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPSNR: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMSE: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSSIM: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], scores[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "ref, degraded, output, scores = predict('images/flowers.bmp')\n",
    "\n",
    "# print all scores for all images\n",
    "print('Degraded Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(scores[0][0], scores[0][1], scores[0][2]))\n",
    "print('Reconstructed Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(scores[1][0], scores[1][1], scores[1][2]))\n",
    "\n",
    "\n",
    "# display images as subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "axs[0].imshow(cv2.cvtColor(ref, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Original')\n",
    "axs[1].imshow(cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title('Degraded')\n",
    "axs[2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "axs[2].set_title('SRCNN')\n",
    "\n",
    "# remove the x and y ticks\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3171016-48f5-4167-bede-908a13b1214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 62s/step\n",
      "Saving baboon.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 66s/step\n",
      "Saving baby_GT.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 114s/step\n",
      "Saving barbara.bmp\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021196893E20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
      "Saving bird_GT.bmp\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021197F87A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 277ms/step\n",
      "Saving butterfly_GT.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step\n",
      "Saving coastguard.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 377ms/step\n",
      "Saving comic.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step\n",
      "Saving face.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847ms/step\n",
      "Saving flowers.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step\n",
      "Saving foreman.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 363ms/step\n",
      "Saving head_GT.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 66s/step\n",
      "Saving lenna.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 102s/step\n",
      "Saving monarch.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 65s/step\n",
      "Saving pepper.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 90s/step\n",
      "Saving ppt3.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320ms/step\n",
      "Saving woman_GT.bmp\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 61s/step\n",
      "Saving zebra.bmp\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('images'):\n",
    "    try:\n",
    "        # Perform super-resolution\n",
    "        result = predict('images/{}'.format(file))\n",
    "        \n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        ref, degraded, output, scores = result\n",
    "        \n",
    "        # Display images as subplots\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "        axs[0].imshow(cv2.cvtColor(ref, cv2.COLOR_BGR2RGB))\n",
    "        axs[0].set_title('Original')\n",
    "        axs[1].imshow(cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB))\n",
    "        axs[1].set_title('Degraded')\n",
    "        axs[1].set(xlabel='PSNR: {}\\nMSE: {}\\nSSIM: {}'.format(scores[0][0], scores[0][1], scores[0][2]))\n",
    "        axs[2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "        axs[2].set_title('SRCNN')\n",
    "        axs[2].set(xlabel='PSNR: {}\\nMSE: {}\\nSSIM: {}'.format(scores[1][0], scores[1][1], scores[1][2]))\n",
    "\n",
    "        # Remove x and y ticks\n",
    "        for ax in axs:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        print('Saving {}'.format(file))\n",
    "        fig.savefig('output/{}.png'.format(os.path.splitext(file)[0]))\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9c17dab-7ac3-407c-b3f5-901dff0b8809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 13:54:41.027 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run F:\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# Streamlit app\n",
    "def main():\n",
    "    st.title(\"Super Resolution with SRCNN\")\n",
    "    \n",
    "    uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\", \"bmp\"])\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        image = Image.open(uploaded_file)\n",
    "        st.image(image, caption='Uploaded Image', use_column_width=True)\n",
    "        \n",
    "        if st.button('Enhance Image'):\n",
    "            # Convert PIL image to OpenCV format\n",
    "            opencv_image = np.array(image.convert('RGB'))\n",
    "            opencv_image = opencv_image[:, :, ::-1]  # Convert RGB to BGR\n",
    "            \n",
    "            # Perform super-resolution\n",
    "            output_image = predict(opencv_image)\n",
    "            \n",
    "            # Display the result\n",
    "            st.image(output_image, caption='Enhanced Image', use_column_width=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e217988-a38e-4e14-b3fd-1e0787d58919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
